!pip install --quiet --upgrade comet_ml
from comet_ml import Experiment
import torch
import torch.cuda
!pip install torchio
import torchio as tio
import torchvision
from torch.nn import CrossEntropyLoss
from torch.cuda.amp import GradScaler
import numpy as np
from pathlib import Path
from functools import partial
import json
!pip install unet
import unet
import sys

from google.colab import drive
drive.mount('/content/drive')

sys.path.insert(0, '/content/drive/My Drive/neuroimage_final_project/path')

from comet_ml import Experiment
import torch
from torch.cuda.amp import GradScaler
import numpy as np
from pathlib import Path
from functools import partial

from mri_segmentation.data import get_data, get_test_data, get_subjects, get_sets, get_loaders
from mri_segmentation.model import get_model
from mri_segmentation.train import train, evaluate
from mri_segmentation.loss import DiceLoss, BoundaryLoss
from mri_segmentation.preprocessing import get_baseline_transforms
from mri_segmentation.metrics import dice_score, hausdorff_score
from mri_segmentation.utils import make_deterministic

experiment = Experiment(
    api_key="1oyzHrJhjofZ4uu1imMYi4y9Y",
    project_name="project-2",
    workspace="d45t4n",
)

root = Path('/content/drive/My Drive/neuroimage_final_project')
data_dir = root / 'la5'
train_data_dir = data_dir / 'train'
test_data_dir = data_dir / 'test'
labels_path = Path('/content/drive/My Drive/neuroimage_final_project/LA5_actual_targets.csv')
distmaps_dir = None

make_deterministic()

if torch.cuda.is_available():
    device = torch.device("cuda:0")
else:
    device = torch.device("cpu")
    
iterator_kwargs = {
    'patch_size': 16,
    'samples_per_volume': 4,
    'max_queue_length': 4,
    'training_batch_size': 1,
    'validation_batch_size': 1,
    'num_training_workers': 2,
    'num_validation_workers': 2,
    'patch_overlap': 0
}

n_classes = 6
spacing = (1., 1., 1.)
num_epochs = 10
key = 'participant_id'

transforms = get_baseline_transforms(n_classes)
train_data_list = get_data(train_data_dir, labels_path, key, distmaps_dir=distmaps_dir, n_classes=n_classes)
train_data_list

test_data_list = get_test_data(test_data_dir, key)
train_subjects = get_subjects(train_data_list['norm'], train_data_list['aseg'])
test_subjects = get_subjects(test_data_list['norm'], test_data_list['aseg'])
train_set, val_set, test_set = get_sets(train_subjects, test_subjects, transforms=transforms)
train_loader, val_loader = get_loaders(train_set, val_set, **iterator_kwargs)

transforms = get_baseline_transforms(n_classes)
train_data_list = get_data(train_data_dir, labels_path, key, distmaps_dir=distmaps_dir, n_classes=n_classes)
test_data_list = get_test_data(test_data_dir, key)
train_subjects = get_subjects(train_data_list['norm'], train_data_list['aseg'])
test_subjects = get_subjects(test_data_list['norm'], test_data_list['aseg'])
train_set, val_set, test_set = get_sets(train_subjects, test_subjects, transforms=transforms)
train_loader, val_loader = get_loaders(train_set, val_set, **iterator_kwargs)

print('Training set:', len(train_set), 'subjects')
print('Validation set:', len(val_set), 'subjects')
print('Testing set:', len(test_set), 'subjects')

torch.cuda.empty_cache()

model_kwargs = {
    'in_channels': 1,
    'out_classes': n_classes,
    'dimensions': 3,
    'num_encoding_blocks': 4,
    'out_channels_first_layer': 8,
    'normalization': 'batch',
    'upsampling_type': 'linear',
    'padding': True,
    'activation': 'PReLU',
    'architecture': 'unet',
    'device': device
}

model = get_model(**model_kwargs)
optimizer = torch.optim.AdamW(model.parameters())
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, threshold=0.01)

_dice_loss = DiceLoss(n_classes)
_boundary_loss = BoundaryLoss(idc=list(range(n_classes)))


def dice_loss(logits, targets):
    return _dice_loss(logits, targets['segm_mask'], softmax=True)


def boundary_loss(logits, targets, alpha=0.01):
    return alpha * _boundary_loss(torch.softmax(logits, dim=1), targets['dist_maps'].argmax(dim=1))


criterions = {'dice': dice_loss}


metrics = {
    **{f'dice_{i}': partial(dice_score, n_classes=n_classes, channel=i) for i in range(n_classes)},
    **{f'hausdorff_{i}': partial(hausdorff_score, spacing=spacing, channel=i) for i in range(n_classes)}
}

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
experiment.set_name("10 epochs, 4 encoding blocks, 8 out, 64 patch, baseline transforms")
weights_stem = 'experiment'
models_dir = Path('/content/drive/My Drive/neuroimage_final_project/models')
model_path = models_dir / f'model_{weights_stem}.pth'

scaler = GradScaler()
train(experiment, num_epochs, train_loader, val_set, model, optimizer, criterions, metrics, scheduler=scheduler,
      save_path=model_path, scaler=scaler, device=device)


model.load_state_dict(torch.load(model_path, map_location=device))
test_scores = evaluate(model, test_set, metrics, **iterator_kwargs)
print(test_scores)
print(f"\nTesting mean score: DICE {np.mean(test_scores['dice']):0.3f}")

experiment.log_metric("avg_test_dice", np.mean(test_scores['dice']))
for i, subject in enumerate(test_subjects):
    experiment.log_metric(f"test_subj_{subject}_dice", test_scores['dice'][i])
    
