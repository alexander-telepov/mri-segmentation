#!pip install --quiet --upgrade comet_ml
from comet_ml import Experiment
import torch
import torch.cuda
#!pip install torchio
import torchio as tio
import torchvision
from torch.nn import CrossEntropyLoss
from torch.cuda.amp import GradScaler
import numpy as np
from pathlib import Path
from functools import partial
import json

from google.colab import drive
drive.mount('/content/drive')

import pathlib
from pathlib import Path

! echo $PYTHONPATH

root = Path('/content/drive/My Drive/neuroimage_final_project')
%set_env PYTHONPATH= '/content/drive/My Drive/neuroimage_final_project/path'

import sys
sys.path.insert(0, '/content/drive/My Drive/neuroimage_final_project/path')
import mri_segmentation

Requirement already satisfied: unet in /usr/local/lib/python3.7/dist-packages (0.7.7)
Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from unet) (1.9.0+cu111)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->unet) (3.7.4.3)

from mri_segmentation.data import get_data, get_subjects, get_sets, get_loaders
from mri_segmentation.model import get_model
from mri_segmentation.train import train, evaluate
from mri_segmentation.preprocessing import get_baseline_transforms
from mri_segmentation.metrics import dice_score, hausdorff_score
from mri_segmentation.utils import make_deterministic, MRI

experiment = Experiment(
    api_key="1oyzHrJhjofZ4uu1imMYi4y9Y",
    project_name="project-2",
    workspace="d45t4n",
)

data_dir = Path('/content/drive/My Drive/neuroimage_final_project/la5')
labels_path = Path('/content/drive/My Drive/neuroimage_final_project/LA5_actual_targets.csv')
distmaps_dir = None

data_dir

make_deterministic()

if torch.cuda.is_available():
    device = torch.device("cuda:0")
else:
    device = torch.device("cpu")
    
iterator_kwargs = {
    'patch_size': 16,
    'samples_per_volume': 4,
    'max_queue_length': 2,
    'training_batch_size': 1,
    'validation_batch_size': 1,
    'num_training_workers': 2,
    'num_validation_workers': 2,
    'patch_overlap': 0
}

n_classes = 6
spacing = (1., 1., 1.)
num_epochs = 10
key = 'participant_id'

with open('/content/drive/My Drive/neuroimage_final_project/test_encoder.json', 'rt') as f:
    test_names = json.load(f)


with open('/content/drive/My Drive/neuroimage_final_project/train_encoder.json', 'rt') as f:
    train_names = json.load(f)
    
transforms = get_baseline_transforms(n_classes)
train_data_list = get_data(data_dir, labels_path, key, distmaps_dir=distmaps_dir, n_classes=n_classes,
                           names=train_names, dropna=False)['aseg'].to_frame().dropna()
test_data_list = get_data(data_dir, labels_path, key, distmaps_dir=distmaps_dir, n_classes=n_classes,
                          names=test_names, dropna=False)['aseg'].to_frame().dropna()
train_subjects = get_subjects(train_data_list['aseg'], train_data_list['aseg'], im_type=tio.LABEL)
test_subjects = get_subjects(test_data_list['aseg'], train_data_list['aseg'], im_type=tio.LABEL)
train_set, val_set, test_set = get_sets(train_subjects, test_subjects, transforms=transforms)
train_loader, val_loader = get_loaders(train_set, val_set, **iterator_kwargs)

print('Training set:', len(train_set), 'subjects')
print('Validation set:', len(val_set), 'subjects')
print('Testing set:', len(test_set), 'subjects')

torch.cuda.empty_cache()

model_kwargs = {
    'in_channels': 6,
    'out_classes': n_classes,
    'dimensions': 3,
    'num_encoding_blocks': 5,
    'out_channels_first_layer': 8,
    'normalization': 'batch',
    'upsampling_type': 'linear',
    'padding': True,
    'activation': 'PReLU',
    'architecture': 'autoencoder',
    'device': device
}

model = get_model(**model_kwargs)
optimizer = torch.optim.AdamW(model.parameters())
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, threshold=0.01)

ce_loss = CrossEntropyLoss()


def cross_entropy(logits, targets):
    return ce_loss(logits, targets['segm_mask'].argmax(dim=1))


criterions = {'cross_entropy': cross_entropy}


metrics = {
    **{f'dice_{i}': partial(dice_score, n_classes=n_classes, channel=i) for i in range(n_classes)},
    **{f'hausdorff_{i}': partial(hausdorff_score, spacing=spacing, channel=i) for i in range(n_classes)}
}

experiment.set_name("train autoencoder")
weights_stem = 'autoencoder'
models_dir = root / 'models'
model_path = models_dir / f'model_{weights_stem}.pth'

scaler = GradScaler()
train(experiment, num_epochs, train_loader, val_set, model, optimizer, criterions, metrics, scheduler=scheduler,
      save_path=model_path, scaler=scaler, device=device)


model.load_state_dict(torch.load(model_path, map_location=device))
test_scores = evaluate(model, test_set, metrics, **iterator_kwargs)
print(test_scores)
print(f"\nTesting mean score: DICE {np.mean(test_scores['dice']):0.3f}")

experiment.log_metric("avg_test_dice", np.mean(test_scores['dice']))
for i, subject in enumerate(test_subjects):
    experiment.log_metric(f"test_subj_{subject}_dice", test_scores['dice'][i])
    
# Unfortunately it is impossible to run autoencoder pipeline without server power: changing parameters, trying to run it locally and in colab result in lack of RAM and restart of kernel.
